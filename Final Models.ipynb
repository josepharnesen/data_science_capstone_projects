{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9(a) - Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages \n",
    "#!pip install tensorflow\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "# build model\n",
    "fnn_model = Sequential([\n",
    "                        Dense(256, activation = 'relu'),\n",
    "                        Dense(128, activation = 'relu'),\n",
    "                        Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9(b) - CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, kernel_size=(3,3), strides=(2,2), padding='same', activation=('relu'), input_shape=(256,256,1)))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "cnn_model.add(Conv2D(64, kernel_size=(2,2), strides=(1,1), padding='same', activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "cnn_model.add(Dense(17, activation='linear'))\n",
    "cnn_model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11 - NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>darkness</th>\n",
       "      <th>earth</th>\n",
       "      <th>god</th>\n",
       "      <th>light</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   darkness  earth  god  light\n",
       "0         0      1    1      0\n",
       "1         1      1    1      0\n",
       "2         0      0    1      2\n",
       "3         1      0    2      2\n",
       "4         1      0    1      1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install gensim\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize, word_tokenize, sent_tokenize\n",
    "from nltk import wordpunct_tokenize, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "wn = WordNetLemmatizer()\n",
    "s1 = 'In the beginning God created the heavens and the earth.'\n",
    "s2 = 'And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.'\n",
    "s3 = 'And God said, Let there be light: and there was light.'\n",
    "s4 = 'And God saw the light, that it was good: and God divided the light from the darkness.'\n",
    "s5 = 'And God called the light Day, and the darkness he called Night.And the evening and the morning were the first day.'\n",
    "\n",
    "# remove punctuation, capitalization, and stopwords\n",
    "s1 = remove_stopwords(s1.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "s2 = remove_stopwords(s2.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "s3 = remove_stopwords(s3.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "s4 = remove_stopwords(s4.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "s5 = remove_stopwords(s5.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# tokenize\n",
    "s1_words = word_tokenize(s1)\n",
    "s2_words = word_tokenize(s2)\n",
    "s3_words = word_tokenize(s3)\n",
    "s4_words = word_tokenize(s4)\n",
    "s5_words = word_tokenize(s5)\n",
    "\n",
    "# lemmatize \n",
    "lemmatized_s1 = ' '.join([wn.lemmatize(word) for word in s1_words])\n",
    "lemmatized_s2 = ' '.join([wn.lemmatize(word) for word in s2_words])\n",
    "lemmatized_s3 = ' '.join([wn.lemmatize(word) for word in s3_words])\n",
    "lemmatized_s4 = ' '.join([wn.lemmatize(word) for word in s4_words])\n",
    "lemmatized_s5 = ' '.join([wn.lemmatize(word) for word in s5_words])\n",
    "\n",
    "corpus = [lemmatized_s1, lemmatized_s2, lemmatized_s3, lemmatized_s4, lemmatized_s5]\n",
    "\n",
    "# create document term matrix \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=2)\n",
    "x_counts = vectorizer.fit_transform(corpus)\n",
    "df_count = pd.DataFrame(x_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "df_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
